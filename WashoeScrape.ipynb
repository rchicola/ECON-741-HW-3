{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WashoeScrape.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNAoe+DEk1NYzkwPFEQFU1w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rchicola/ECON-741-HW-3/blob/master/WashoeScrape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoCniNx3_V0r"
      },
      "source": [
        "### Clark County Assessor's Website scrapping property data by A.P.N. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9lPERRGo3Ka"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj1CJZwd_PPJ"
      },
      "source": [
        "#Beautiful Soup DOCUMENTATION\r\n",
        "#https://www.crummy.com/software/BeautifulSoup/bs4/doc/\r\n",
        "\r\n",
        "import requests #pip install requests\r\n",
        "from bs4 import BeautifulSoup as bs # pip install beautifulsoup4\r\n",
        "# Leverage with 'reg ex library' = 'Regular expression operations' = re library\r\n",
        "import re \r\n",
        "# For data analysis and manipulation\r\n",
        "import pandas as pd # pip install pandas\r\n",
        "\r\n",
        "#################################################################################################\r\n",
        "#START LOOP\r\n",
        "#################################################################################################\r\n",
        "\r\n",
        "# Using a sample APN (1st row from spreadsheet APN#  16327610009 in the Property Search by APN utility)\r\n",
        "#See ClarkCounty_AddressesPrecints excel spreadsheet\r\n",
        "#Load the webpage content\r\n",
        "load = requests.get(\"https://maps.clarkcountynv.gov/assessor/AssessorParcelDetail/ParcelDetail.aspx?hdnParcel=16327610009&hdnInstance=pcl7\")\r\n",
        "\r\n",
        "#Convert to a beautiful soup object\r\n",
        "soup = bs(load.content)\r\n",
        "#print(soup.prettify())\r\n",
        "\r\n",
        "#################################################################################################\r\n",
        "#Put Property data into variables using the id and .contents to get just value and not id itself\r\n",
        "\r\n",
        "APN_value = soup.find(id = \"lblParcel\").contents\r\n",
        "Owner = soup.find(id = \"lblOwner1\").contents \r\n",
        "\r\n",
        "#ADDRESS \r\n",
        "Address1 = soup.find(id = \"lblAddr1\").contents \r\n",
        "Address2 = soup.find(id = \"lblAddr2\").contents \r\n",
        "Address3 = soup.find(id = \"lblAddr3\").contents \r\n",
        "Address4 = soup.find(id = \"lblAddr4\").contents \r\n",
        "Address5 = soup.find(id = \"lblAddr5\").contents \r\n",
        "\r\n",
        "Location_Add = soup.find(id = \"lblLocation\").contents  \r\n",
        "City_Unincorp_town = soup.find(id = \"lblTown\").contents \r\n",
        "\r\n",
        "Assess_Desc1 = soup.find(id = \"recDesc1\").contents \r\n",
        "Assess_Desc2 = soup.find(id = \"recDesc2\").contents \r\n",
        "Assess_Desc3 = soup.find(id = \"recDesc3\").contents \r\n",
        "\r\n",
        "Record_doc_No = soup.find(id = \"RecDoc\").contents  \r\n",
        "Record_date =  soup.find(id = \"lblRecDate\").contents\r\n",
        "Vesting =  soup.find(id = \"lblVest\").contents\r\n",
        "\r\n",
        "#Appraisal\r\n",
        "Tax_District = soup.find(id = \"lblTaxDist\").contents \r\n",
        "Appraisal_YR = soup.find(id = \"lblApprYr\").contents \r\n",
        "\r\n",
        "#REAL PROPERTY ASSESSED VALUE  (Two Fiscal periods 2020-2021 and 2021-2022)\r\n",
        "Land_20_21 = soup.find(id = \"lblLand1\").contents \r\n",
        "Land_21_22 = soup.find(id = \"lblLand2\").contents \r\n",
        "#\r\n",
        "Improvements_20_21 = soup.find(id = \"lblLand1\").contents \r\n",
        "Improvements_21_22 = soup.find(id = \"lblLand2\").contents \r\n",
        "#\r\n",
        "Gross_Assessed_Sub_20_21 = soup.find(id = \"lblGross1\").contents \r\n",
        "Gross_Assessed_Sub_21_22 = soup.find(id = \"lblGross2\").contents \r\n",
        "#\r\n",
        "Taxable_LnI_20_21 = soup.find(id = \"lblTaxVal1\").contents \r\n",
        "Taxable_LnI_21_22 = soup.find(id = \"lblTaxVal2\").contents \r\n",
        "#\r\n",
        "Tot_Ass_Val_20_21 = soup.find(id = \"lblTAssessed1\").contents \r\n",
        "Tot_Ass_Val_21_22 = soup.find(id = \"lblTAssessed2\").contents \r\n",
        "#\r\n",
        "Tot_Tax_Val__20_21 = soup.find(id = \"lblTTaxable1\").contents \r\n",
        "Tot_Tax_Val_21_22  = soup.find(id = \"lblTTaxable2\").contents \r\n",
        "\r\n",
        "#Estimated Lot Size and Apprasial Info\r\n",
        "Est_Size = soup.find(id = \"lblAcres\").contents \r\n",
        "Org_Construct_Yr  = soup.find(id = \"lblConstrYr\").contents \r\n",
        "Last_Sale_Price = soup.find(id = \"lblSalePrice\").contents \r\n",
        "Month_YR = soup.find(id = \"lblSaleDate\").contents \r\n",
        "Sale_Type = soup.find(id = \"lblSaleType\").contents \r\n",
        "Land_use =soup.find(id = \"lblLandUse\").contents \r\n",
        "Dwelling_unit = soup.find(id = \"lblUnits\").contents \r\n",
        "\r\n",
        "#Primary Residential Structure\r\n",
        "Floor1_sqr = soup.find(id = \"lblFirstFloor\").contents \r\n",
        "Floor2_sqr = soup.find(id = \"lblSecondFloor\").contents \r\n",
        "Floor3_sqr = soup.find(id = \"lblThirdFloor\").contents \r\n",
        "\r\n",
        "Unfin_base = soup.find(id = \"lblUnfinishedBasement\").contents \r\n",
        "Fin_base = soup.find(id = \"lblFinishedBasement\").contents\r\n",
        "Base_Garage = soup.find(id = \"lblBasementGarage\").contents \r\n",
        "Tot_Garage = soup.find(id = \"lblGarage\").contents \r\n",
        "\r\n",
        "Casita_sq = soup.find(id = \"lblCasita\").contents \r\n",
        "Carport = soup.find(id = \"lblCarPort\").contents\r\n",
        "Style = soup.find(id = \"lblStories\").contents\r\n",
        "Bedrooms  = soup.find(id = \"lblBedrooms\").contents\r\n",
        "Bathrooms = soup.find(id = \"lblBath\").contents\r\n",
        "\r\n",
        "ADDN_CONV = soup.find(id = \"lblAddition\").contents\r\n",
        "Pool =  soup.find(id = \"lblPool\").contents\r\n",
        "Spa = soup.find(id = \"lblSpa\").contents\r\n",
        "Construct_type = soup.find(id = \"lblConstType\" ).contents\r\n",
        "Roof_type = soup.find(id = \"lblRoof\" ).contents\r\n",
        "Fireplace = soup.find(id = \"lblFireplace\").contents\r\n",
        "\r\n",
        "#################################################################################################\r\n",
        "#Create dataframe to put this APN observation row of data on variables scraped , using the APN # to label \r\n",
        "#df_1 = pd.Dataframe({\"16327610009\":[Address1,Address2,Address3,Address4,Address5, ]})\r\n",
        "\r\n",
        "#Actually we should probably put each row observation into an list , have a i_list for each i individual APN \r\n",
        "APN_list_16327610009 = [APN_value, Owner, Address1,Address2,Address3,Address4,Address5, Location_Add, City_Unincorp_town, Assess_Desc1, Assess_Desc2, Assess_Desc3, Record_doc_No, Record_date, Vesting, \r\n",
        "                        Tax_District,Appraisal_YR,Land_20_21, Improvements_21_22, Gross_Assessed_Sub_20_21, Gross_Assessed_Sub_21_22 ,\r\n",
        "                        Taxable_LnI_20_21, Tot_Ass_Val_20_21, Tot_Ass_Val_21_22, Tot_Tax_Val__20_21, Tot_Tax_Val_21_22, Est_Size, \r\n",
        "                         Org_Construct_Yr, Last_Sale_Price, Month_YR, Sale_Type, Land_use, Dwelling_unit, Floor1_sqr, Floor2_sqr, Floor3_sqr,     \r\n",
        "                        Unfin_base, Fin_base, Base_Garage, Tot_Garage, Casita_sq, Carport, Style, Bedrooms, Bathrooms,ADDN_CONV, Pool, Spa, Construct_type,\r\n",
        "                       Roof_type, Fireplace] \r\n",
        "APN_list_16327610009\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHe0JgNAJ2WZ"
      },
      "source": [
        "# Test LOOP SANDBOX\r\n",
        "\r\n",
        "#Beautiful Soup DOCUMENTATION\r\n",
        "#https://www.crummy.com/software/BeautifulSoup/bs4/doc/\r\n",
        "#See ClarkCounty_AddressesPrecints excel spreadsheet for APNs\r\n",
        "\r\n",
        "import requests #pip install requests\r\n",
        "from bs4 import BeautifulSoup as bs # pip install beautifulsoup4\r\n",
        "# Leverage with 'reg ex library' = 'Regular expression operations' = re library\r\n",
        "import re \r\n",
        "# For data analysis and manipulation\r\n",
        "import pandas as pd # pip install pandas\r\n",
        "#For multi-dimensional arrays\r\n",
        "import numpy as np\r\n",
        "# CURRENT WORKING DIRECTORY\r\n",
        "import os\r\n",
        "os.getcwd()\r\n",
        "# For String Manipulation\r\n",
        "import string\r\n",
        "\r\n",
        "#UPLOAD APN FILE\r\n",
        "APN_list = []\r\n",
        "APN_list = pd.read_excel('/content/APN_Vector.xlsx', header=None)\r\n",
        "\r\n",
        "#UPLOAD APN URL FILE\r\n",
        "APN_URL_list=[]\r\n",
        "APN_URL_list = pd.read_excel('/content/APN URL VECTOR.xlsx', header=None)\r\n",
        "\r\n",
        "Results_Object = [\"APN_value\",\"Owner\"]\r\n",
        "\r\n",
        "\r\n",
        "for URL in APN_URL_list :\r\n",
        "  load = requests.get(URL)\r\n",
        "  #Convert to a beautiful soup object\r\n",
        "  soup = bs(load.content)\r\n",
        "  #Put Property data into variables using the id and .contents to get just value and not id itself\r\n",
        "  APN_value = soup.find(id = \"lblParcel\").contents\r\n",
        "  Owner = soup.find(id = \"lblOwner1\").contents \r\n",
        "  Row_list = [APN_value, Owner]\r\n",
        "  for obj in Results_Object :\r\n",
        "   Results_Object.append(Row_list)          \r\n",
        "  \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#print(soup.prettify())\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDp_JutFcpZq"
      },
      "source": [
        "# Test LOOP SANDBOX 2\r\n",
        "import requests #pip install requests\r\n",
        "from bs4 import BeautifulSoup as bs # pip install beautifulsoup4\r\n",
        "# Leverage with 'reg ex library' = 'Regular expression operations' = re library\r\n",
        "import re \r\n",
        "# For data analysis and manipulation\r\n",
        "import pandas as pd # pip install pandas\r\n",
        "#For multi-dimensional arrays\r\n",
        "import numpy as np\r\n",
        "# CURRENT WORKING DIRECTORY\r\n",
        "import os\r\n",
        "os.getcwd()\r\n",
        "# For String Manipulation\r\n",
        "import string\r\n",
        "\r\n",
        "\r\n",
        "# Trying to figure out the body of loop, suppose we want to pick 2nd APN in vector and get associated URL\r\n",
        "WEB_SCRAPE_RESULTS = pd.DataFrame(columns=('APN_value','Owner'))\r\n",
        "\r\n",
        "#UPLOAD APN FILE\r\n",
        "df1 = []\r\n",
        "df1 = pd.read_excel('/content/APN_Vector.xlsx', header=None)\r\n",
        "\r\n",
        "#UPLOAD APN URL FILE\r\n",
        "df2=[]\r\n",
        "df2 = pd.read_excel('/content/APN URL VECTOR.xlsx', header=None)\r\n",
        "\r\n",
        "df2.iloc[2,0]\r\n",
        "\r\n",
        "\r\n",
        "for i in range(5):\r\n",
        "  load = requests.get(i)\r\n",
        "  soup = bs(load.content)\r\n",
        "  APN_value = soup.find(id = \"lblParcel\").contents\r\n",
        "  Owner = soup.find(id = \"lblOwner1\").contents \r\n",
        "  APN_Row_i_To_Add = [APN_value, Owner]\r\n",
        "  WEB_SCRAPE_RESULTS.append(APN_Row_i_To_Add)\r\n",
        "\r\n",
        "WEB_SCRAPE_RESULTS\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE6z7vw2ijFq"
      },
      "source": [
        "\n",
        "# To connect Jupyter to a Gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epsH53oNrpr7",
        "outputId": "5ca22a76-7c4a-42b3-d51c-b2ee699acae2"
      },
      "source": [
        "#SandBox 3  ---SMALL SCALE SAMPLE VECTOR LIST\r\n",
        "\r\n",
        "import requests #pip install requests\r\n",
        "from bs4 import BeautifulSoup as bs # pip install beautifulsoup4\r\n",
        "# Leverage with 'reg ex library' = 'Regular expression operations' = re library\r\n",
        "import re \r\n",
        "# For data analysis and manipulation\r\n",
        "import pandas as pd # pip install pandas\r\n",
        "#For multi-dimensional arrays\r\n",
        "import numpy as np\r\n",
        "# CURRENT WORKING DIRECTORY\r\n",
        "import os\r\n",
        "os.getcwd()\r\n",
        "# For String Manipulation\r\n",
        "import string\r\n",
        "# For Excel Output\r\n",
        "#!pip install xlsxwriter\r\n",
        "import xlsxwriter\r\n",
        "\r\n",
        "#UPLOAD APN FILE AND CREATE APN LIST\r\n",
        "APN_sample = []\r\n",
        "APN_sample = pd.read_excel('/content/APN_Vector_sample.xlsx')\r\n",
        "APN_list = list(APN_sample['APN'])\r\n",
        "\r\n",
        "#UPLOAD APN URL FILE\r\n",
        "APN_URL_sample = []\r\n",
        "APN_URL_sample  = pd.read_excel('/content/APN URL VECTOR sample.xlsx')\r\n",
        "APN_URL_list = list(APN_URL_sample ['APN URL'])\r\n",
        "\r\n",
        "#https://stackoverflow.com/questions/63292489/combine-dataframes-at-end-of-a-loop-for-web-page-scrape\r\n",
        "\r\n",
        "Results_Object = None\r\n",
        "\r\n",
        "for URL in APN_URL_list:\r\n",
        "  #Connect to URL for HTML page source code\r\n",
        "  load = requests.get(URL)\r\n",
        "  #Convert to a beautiful soup object\r\n",
        "  soup = bs(load.content)\r\n",
        "  #Scrape values from HTML using id\r\n",
        "  APN_value = soup.find(id = \"lblParcel\").contents\r\n",
        "  Owner = soup.find(id = \"lblOwner1\").contents \r\n",
        "  Last_Sale_Price = soup.find(id = \"lblSalePrice\").contents\r\n",
        "\r\n",
        "  Row_list = [APN_value, Owner, Last_Sale_Price]\r\n",
        "\r\n",
        "  if Results_Object == None:\r\n",
        "    Results_Object = Row_list # To start first row\r\n",
        "  else: #Append the new row 2nd, 3rd, etc.\r\n",
        "    Results_Object.append(Row_list)\r\n",
        "\r\n",
        "#Create dataframe object from list of lists created by loop\r\n",
        "Web_Scrape_Results = pd.DataFrame.from_records(Results_Object)\r\n",
        "#Create Excel Writer object for file title\r\n",
        "writer = pd.ExcelWriter('Washoe_Web_Scrape_Small_Scale.xlsx')\r\n",
        "#Write dataframe to excel\r\n",
        "Web_Scrape_Results.to_excel(writer)\r\n",
        "#Save the Excel\r\n",
        "writer.save()\r\n",
        "print('Dataframe successfully written to Excel')\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataframe successfully written to Excel\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "ZFti7foWqmUs",
        "outputId": "505446d5-5814-4ee1-eacf-38fe4263488b"
      },
      "source": [
        "#SandBox 4 - Medium Scale Test: Full APN URL Vector, but still only scrapping a few things\r\n",
        "\r\n",
        "\r\n",
        "import requests #pip install requests\r\n",
        "from bs4 import BeautifulSoup as bs # pip install beautifulsoup4\r\n",
        "# Leverage with 'reg ex library' = 'Regular expression operations' = re library\r\n",
        "import re \r\n",
        "# For data analysis and manipulation\r\n",
        "import pandas as pd # pip install pandas\r\n",
        "#For multi-dimensional arrays\r\n",
        "import numpy as np\r\n",
        "# CURRENT WORKING DIRECTORY\r\n",
        "import os\r\n",
        "os.getcwd()\r\n",
        "# For String Manipulation\r\n",
        "import string\r\n",
        "# For Excel Output\r\n",
        "#!pip install xlsxwriter\r\n",
        "import xlsxwriter\r\n",
        "\r\n",
        "#UPLOAD APN FILE AND CREATE APN LIST\r\n",
        "APN_vector = []\r\n",
        "APN_vector = pd.read_excel('/content/APN_Vector.xlsx')\r\n",
        "APN_list = list(APN_vector['APN'])\r\n",
        "\r\n",
        "#UPLOAD APN URL FILE\r\n",
        "APN_URL = []\r\n",
        "APN_URL  = pd.read_excel('/content/APN URL VECTOR.xlsx')\r\n",
        "APN_URL_list = list(APN_URL['APN URL'])\r\n",
        "\r\n",
        "Results_Object = None\r\n",
        "\r\n",
        "for URL in APN_URL_list:\r\n",
        "  #Connect to URL for HTML page source code\r\n",
        "  load = requests.get(URL)\r\n",
        "  #Convert to a beautiful soup object\r\n",
        "  soup = bs(load.content)\r\n",
        "  #Scrape values from HTML using id\r\n",
        "  APN_value = soup.find(id = \"lblParcel\").contents\r\n",
        "  Owner = soup.find(id = \"lblOwner1\").contents \r\n",
        "  Last_Sale_Price = soup.find(id = \"lblSalePrice\").contents\r\n",
        "  #Create a list object for all the scraped items\r\n",
        "  Row_list = [APN_value, Owner, Last_Sale_Price]\r\n",
        "\r\n",
        "  if Results_Object == None:\r\n",
        "    Results_Object = Row_list # To start first row\r\n",
        "  else: #Append the new row 2nd, 3rd, etc.\r\n",
        "    Results_Object.append(Row_list)\r\n",
        "\r\n",
        "#Create dataframe object from list of lists created by loop\r\n",
        "Web_Scrape_Results = pd.DataFrame.from_records(Results_Object)\r\n",
        "#Create Excel Writer object for file title\r\n",
        "writer = pd.ExcelWriter('Washoe_Web_Scrape_Medium_scale.xlsx')\r\n",
        "#Write dataframe to excel\r\n",
        "Web_Scrape_Results.to_excel(writer)\r\n",
        "#Save the Excel\r\n",
        "writer.save()\r\n",
        "print('Dataframe successfully written to Excel')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-1fc5b8b8606b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;31m#Scrape values from HTML using id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0mAPN_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"lblParcel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0mOwner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"lblOwner1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m   \u001b[0mLast_Sale_Price\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"lblSalePrice\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;31m#Create a list object for all the scraped items\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'contents'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6-jf_ZJpdbW",
        "outputId": "d703b941-e0c9-49ab-e852-b79af11183bd"
      },
      "source": [
        "#SandBox 5 - Medium Scale Test_B:\r\n",
        "# Use APN URL Sample Vector, but try to include all webscrape items\r\n",
        "\r\n",
        "\r\n",
        "import requests #pip install requests\r\n",
        "from bs4 import BeautifulSoup as bs # pip install beautifulsoup4\r\n",
        "# Leverage with 'reg ex library' = 'Regular expression operations' = re library\r\n",
        "import re \r\n",
        "# For data analysis and manipulation\r\n",
        "import pandas as pd # pip install pandas\r\n",
        "#For multi-dimensional arrays\r\n",
        "import numpy as np\r\n",
        "# CURRENT WORKING DIRECTORY\r\n",
        "import os\r\n",
        "os.getcwd()\r\n",
        "# For String Manipulation\r\n",
        "import string\r\n",
        "# For Excel Output\r\n",
        "!pip install xlsxwriter\r\n",
        "import xlsxwriter\r\n",
        "\r\n",
        "\r\n",
        "#UPLOAD APN URL FILE\r\n",
        "APN_URL_sample = []\r\n",
        "APN_URL_sample  = pd.read_excel('/content/APN URL VECTOR sample.xlsx')\r\n",
        "APN_URL_list = list(APN_URL_sample ['APN URL'])\r\n",
        "\r\n",
        "Results_Object = None\r\n",
        "\r\n",
        "for URL in APN_URL_list:\r\n",
        "  #Connect to URL for HTML page source code\r\n",
        "  load = requests.get(URL)\r\n",
        "  #Convert to a beautiful soup object\r\n",
        "  soup = bs(load.content)\r\n",
        "\r\n",
        "  ###################################################\r\n",
        "  #Scrape values from HTML using id\r\n",
        "  APN_value = soup.find(id = \"lblParcel\").contents\r\n",
        "  Owner = soup.find(id = \"lblOwner1\").contents \r\n",
        "\r\n",
        "  #ADDRESS \r\n",
        "  Address1 = soup.find(id = \"lblAddr1\").contents \r\n",
        "  Address2 = soup.find(id = \"lblAddr2\").contents \r\n",
        "  Address3 = soup.find(id = \"lblAddr3\").contents \r\n",
        "  Address4 = soup.find(id = \"lblAddr4\").contents \r\n",
        "  Address5 = soup.find(id = \"lblAddr5\").contents \r\n",
        "\r\n",
        "  Location_Add = soup.find(id = \"lblLocation\").contents  \r\n",
        "  City_Unincorp_town = soup.find(id = \"lblTown\").contents \r\n",
        "\r\n",
        "  Assess_Desc1 = soup.find(id = \"recDesc1\").contents \r\n",
        "  Assess_Desc2 = soup.find(id = \"recDesc2\").contents \r\n",
        "  Assess_Desc3 = soup.find(id = \"recDesc3\").contents \r\n",
        "\r\n",
        "  Record_doc_No = soup.find(id = \"RecDoc\").contents  \r\n",
        "  Record_date =  soup.find(id = \"lblRecDate\").contents\r\n",
        "  Vesting =  soup.find(id = \"lblVest\").contents\r\n",
        "\r\n",
        "  #Appraisal\r\n",
        "  Tax_District = soup.find(id = \"lblTaxDist\").contents \r\n",
        "  Appraisal_YR = soup.find(id = \"lblApprYr\").contents \r\n",
        "\r\n",
        "  #REAL PROPERTY ASSESSED VALUE  (Two Fiscal periods 2020-2021 and 2021-2022)\r\n",
        "  Land_20_21 = soup.find(id = \"lblLand1\").contents \r\n",
        "  Land_21_22 = soup.find(id = \"lblLand2\").contents \r\n",
        "  #\r\n",
        "  Improvements_20_21 = soup.find(id = \"lblLand1\").contents \r\n",
        "  Improvements_21_22 = soup.find(id = \"lblLand2\").contents \r\n",
        "  #\r\n",
        "  Gross_Assessed_Sub_20_21 = soup.find(id = \"lblGross1\").contents \r\n",
        "  Gross_Assessed_Sub_21_22 = soup.find(id = \"lblGross2\").contents \r\n",
        "  #\r\n",
        "  Taxable_LnI_20_21 = soup.find(id = \"lblTaxVal1\").contents \r\n",
        "  Taxable_LnI_21_22 = soup.find(id = \"lblTaxVal2\").contents \r\n",
        "  #\r\n",
        "  Tot_Ass_Val_20_21 = soup.find(id = \"lblTAssessed1\").contents \r\n",
        "  Tot_Ass_Val_21_22 = soup.find(id = \"lblTAssessed2\").contents \r\n",
        "  #\r\n",
        "  Tot_Tax_Val__20_21 = soup.find(id = \"lblTTaxable1\").contents \r\n",
        "  Tot_Tax_Val_21_22  = soup.find(id = \"lblTTaxable2\").contents \r\n",
        "\r\n",
        "  #Estimated Lot Size and Apprasial Info\r\n",
        "  Est_Size = soup.find(id = \"lblAcres\").contents \r\n",
        "  Org_Construct_Yr  = soup.find(id = \"lblConstrYr\").contents \r\n",
        "  Last_Sale_Price = soup.find(id = \"lblSalePrice\").contents \r\n",
        "  Month_YR = soup.find(id = \"lblSaleDate\").contents \r\n",
        "  Sale_Type = soup.find(id = \"lblSaleType\").contents \r\n",
        "  Land_use =soup.find(id = \"lblLandUse\").contents \r\n",
        "  Dwelling_unit = soup.find(id = \"lblUnits\").contents \r\n",
        "\r\n",
        "  #Primary Residential Structure\r\n",
        "  Floor1_sqr = soup.find(id = \"lblFirstFloor\").contents \r\n",
        "  Floor2_sqr = soup.find(id = \"lblSecondFloor\").contents \r\n",
        "  Floor3_sqr = soup.find(id = \"lblThirdFloor\").contents \r\n",
        "\r\n",
        "  Unfin_base = soup.find(id = \"lblUnfinishedBasement\").contents \r\n",
        "  Fin_base = soup.find(id = \"lblFinishedBasement\").contents\r\n",
        "  Base_Garage = soup.find(id = \"lblBasementGarage\").contents \r\n",
        "  Tot_Garage = soup.find(id = \"lblGarage\").contents \r\n",
        "\r\n",
        "  Casita_sq = soup.find(id = \"lblCasita\").contents \r\n",
        "  Carport = soup.find(id = \"lblCarPort\").contents\r\n",
        "  Style = soup.find(id = \"lblStories\").contents\r\n",
        "  Bedrooms  = soup.find(id = \"lblBedrooms\").contents\r\n",
        "  Bathrooms = soup.find(id = \"lblBath\").contents\r\n",
        "\r\n",
        "  ADDN_CONV = soup.find(id = \"lblAddition\").contents\r\n",
        "  Pool =  soup.find(id = \"lblPool\").contents\r\n",
        "  Spa = soup.find(id = \"lblSpa\").contents\r\n",
        "  Construct_type = soup.find(id = \"lblConstType\" ).contents\r\n",
        "  Roof_type = soup.find(id = \"lblRoof\" ).contents\r\n",
        "  Fireplace = soup.find(id = \"lblFireplace\").contents\r\n",
        "###################################################\r\n",
        "\r\n",
        "  #Create a list object for all the scraped items\r\n",
        "  Row_list = [APN_value, Owner, Address1,Address2,Address3,Address4,Address5, Location_Add, City_Unincorp_town, Assess_Desc1, Assess_Desc2, Assess_Desc3, Record_doc_No, Record_date, Vesting, \r\n",
        "                        Tax_District,Appraisal_YR,Land_20_21, Improvements_21_22, Gross_Assessed_Sub_20_21, Gross_Assessed_Sub_21_22 ,\r\n",
        "                        Taxable_LnI_20_21, Tot_Ass_Val_20_21, Tot_Ass_Val_21_22, Tot_Tax_Val__20_21, Tot_Tax_Val_21_22, Est_Size, \r\n",
        "                         Org_Construct_Yr, Last_Sale_Price, Month_YR, Sale_Type, Land_use, Dwelling_unit, Floor1_sqr, Floor2_sqr, Floor3_sqr,     \r\n",
        "                        Unfin_base, Fin_base, Base_Garage, Tot_Garage, Casita_sq, Carport, Style, Bedrooms, Bathrooms,ADDN_CONV, Pool, Spa, Construct_type,\r\n",
        "                       Roof_type, Fireplace]\r\n",
        "\r\n",
        "  if Results_Object == None :\r\n",
        "    Results_Object = Row_list # To start first row\r\n",
        "  else: #Append the new row 2nd, 3rd, etc.\r\n",
        "    Results_Object.append(Row_list)\r\n",
        "\r\n",
        "#Create dataframe object from list of lists created by loop\r\n",
        "Web_Scrape_Results = pd.DataFrame.from_records(Results_Object)\r\n",
        "#Create Excel Writer object for file title\r\n",
        "writer = pd.ExcelWriter('Washoe_Web_Scrape_MediumB_scale.xlsx')\r\n",
        "#Write dataframe to excel\r\n",
        "Web_Scrape_Results.to_excel(writer)\r\n",
        "#Save the Excel\r\n",
        "writer.save()\r\n",
        "print('Dataframe successfully written to Excel')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.6/dist-packages (1.3.7)\n",
            "Dataframe successfully written to Excel\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCAmWO-eLrF0"
      },
      "source": [
        "# Run batches of APNs\r\n",
        "#SandBox 5 - Medium Scale Test_B:\r\n",
        "# Use APN URL Sample Vector, but try to include all webscrape items\r\n",
        "\r\n",
        "\r\n",
        "import requests #pip install requests\r\n",
        "from bs4 import BeautifulSoup as bs # pip install beautifulsoup4\r\n",
        "# Leverage with 'reg ex library' = 'Regular expression operations' = re library\r\n",
        "import re \r\n",
        "# For data analysis and manipulation\r\n",
        "import pandas as pd # pip install pandas\r\n",
        "#For multi-dimensional arrays\r\n",
        "import numpy as np\r\n",
        "# CURRENT WORKING DIRECTORY\r\n",
        "import os\r\n",
        "os.getcwd()\r\n",
        "# For String Manipulation\r\n",
        "import string\r\n",
        "# For Excel Output\r\n",
        "!pip install xlsxwriter\r\n",
        "import xlsxwriter\r\n",
        "\r\n",
        "\r\n",
        "#UPLOAD APN URL FILE\r\n",
        "APN_URL_sample = []\r\n",
        "APN_URL_sample  = pd.read_excel('/content/APN URL VECTOR.xlsx')\r\n",
        "APN_URL_list = list(APN_URL_sample ['APN URL'])\r\n",
        "\r\n",
        "Results_Object = None\r\n",
        "\r\n",
        "for URL in APN_URL_list:\r\n",
        "  #Connect to URL for HTML page source code\r\n",
        "  load = requests.get(URL)\r\n",
        "  #Convert to a beautiful soup object\r\n",
        "  soup = bs(load.content)\r\n",
        "\r\n",
        "  ###################################################\r\n",
        "  #Scrape values from HTML using id\r\n",
        "  APN_value = soup.find(id = \"lblParcel\").contents\r\n",
        "  Owner = soup.find(id = \"lblOwner1\").contents \r\n",
        "\r\n",
        "  #ADDRESS \r\n",
        "  Address1 = soup.find(id = \"lblAddr1\").contents \r\n",
        "  Address2 = soup.find(id = \"lblAddr2\").contents \r\n",
        "  Address3 = soup.find(id = \"lblAddr3\").contents \r\n",
        "  Address4 = soup.find(id = \"lblAddr4\").contents \r\n",
        "  Address5 = soup.find(id = \"lblAddr5\").contents \r\n",
        "\r\n",
        "  Location_Add = soup.find(id = \"lblLocation\").contents  \r\n",
        "  City_Unincorp_town = soup.find(id = \"lblTown\").contents \r\n",
        "\r\n",
        "  Assess_Desc1 = soup.find(id = \"recDesc1\").contents \r\n",
        "  Assess_Desc2 = soup.find(id = \"recDesc2\").contents \r\n",
        "  Assess_Desc3 = soup.find(id = \"recDesc3\").contents \r\n",
        "\r\n",
        "  Record_doc_No = soup.find(id = \"RecDoc\").contents  \r\n",
        "  Record_date =  soup.find(id = \"lblRecDate\").contents\r\n",
        "  Vesting =  soup.find(id = \"lblVest\").contents\r\n",
        "\r\n",
        "  #Appraisal\r\n",
        "  Tax_District = soup.find(id = \"lblTaxDist\").contents \r\n",
        "  Appraisal_YR = soup.find(id = \"lblApprYr\").contents \r\n",
        "\r\n",
        "  #REAL PROPERTY ASSESSED VALUE  (Two Fiscal periods 2020-2021 and 2021-2022)\r\n",
        "  Land_20_21 = soup.find(id = \"lblLand1\").contents \r\n",
        "  Land_21_22 = soup.find(id = \"lblLand2\").contents \r\n",
        "  #\r\n",
        "  Improvements_20_21 = soup.find(id = \"lblLand1\").contents \r\n",
        "  Improvements_21_22 = soup.find(id = \"lblLand2\").contents \r\n",
        "  #\r\n",
        "  Gross_Assessed_Sub_20_21 = soup.find(id = \"lblGross1\").contents \r\n",
        "  Gross_Assessed_Sub_21_22 = soup.find(id = \"lblGross2\").contents \r\n",
        "  #\r\n",
        "  Taxable_LnI_20_21 = soup.find(id = \"lblTaxVal1\").contents \r\n",
        "  Taxable_LnI_21_22 = soup.find(id = \"lblTaxVal2\").contents \r\n",
        "  #\r\n",
        "  Tot_Ass_Val_20_21 = soup.find(id = \"lblTAssessed1\").contents \r\n",
        "  Tot_Ass_Val_21_22 = soup.find(id = \"lblTAssessed2\").contents \r\n",
        "  #\r\n",
        "  Tot_Tax_Val__20_21 = soup.find(id = \"lblTTaxable1\").contents \r\n",
        "  Tot_Tax_Val_21_22  = soup.find(id = \"lblTTaxable2\").contents \r\n",
        "\r\n",
        "  #Estimated Lot Size and Apprasial Info\r\n",
        "  Est_Size = soup.find(id = \"lblAcres\").contents \r\n",
        "  Org_Construct_Yr  = soup.find(id = \"lblConstrYr\").contents \r\n",
        "  Last_Sale_Price = soup.find(id = \"lblSalePrice\").contents \r\n",
        "  Month_YR = soup.find(id = \"lblSaleDate\").contents \r\n",
        "  Sale_Type = soup.find(id = \"lblSaleType\").contents \r\n",
        "  Land_use =soup.find(id = \"lblLandUse\").contents \r\n",
        "  Dwelling_unit = soup.find(id = \"lblUnits\").contents \r\n",
        "\r\n",
        "  #Primary Residential Structure\r\n",
        "  Floor1_sqr = soup.find(id = \"lblFirstFloor\").contents \r\n",
        "  Floor2_sqr = soup.find(id = \"lblSecondFloor\").contents \r\n",
        "  Floor3_sqr = soup.find(id = \"lblThirdFloor\").contents \r\n",
        "\r\n",
        "  Unfin_base = soup.find(id = \"lblUnfinishedBasement\").contents \r\n",
        "  Fin_base = soup.find(id = \"lblFinishedBasement\").contents\r\n",
        "  Base_Garage = soup.find(id = \"lblBasementGarage\").contents \r\n",
        "  Tot_Garage = soup.find(id = \"lblGarage\").contents \r\n",
        "\r\n",
        "  Casita_sq = soup.find(id = \"lblCasita\").contents \r\n",
        "  Carport = soup.find(id = \"lblCarPort\").contents\r\n",
        "  Style = soup.find(id = \"lblStories\").contents\r\n",
        "  Bedrooms  = soup.find(id = \"lblBedrooms\").contents\r\n",
        "  Bathrooms = soup.find(id = \"lblBath\").contents\r\n",
        "\r\n",
        "  ADDN_CONV = soup.find(id = \"lblAddition\").contents\r\n",
        "  Pool =  soup.find(id = \"lblPool\").contents\r\n",
        "  Spa = soup.find(id = \"lblSpa\").contents\r\n",
        "  Construct_type = soup.find(id = \"lblConstType\" ).contents\r\n",
        "  Roof_type = soup.find(id = \"lblRoof\" ).contents\r\n",
        "  Fireplace = soup.find(id = \"lblFireplace\").contents\r\n",
        "###################################################\r\n",
        "\r\n",
        "  #Create a list object for all the scraped items\r\n",
        "  Row_list = [APN_value, Owner, Address1,Address2,Address3,Address4,Address5, Location_Add, City_Unincorp_town, Assess_Desc1, Assess_Desc2, Assess_Desc3, Record_doc_No, Record_date, Vesting, \r\n",
        "                        Tax_District,Appraisal_YR,Land_20_21, Improvements_21_22, Gross_Assessed_Sub_20_21, Gross_Assessed_Sub_21_22 ,\r\n",
        "                        Taxable_LnI_20_21, Tot_Ass_Val_20_21, Tot_Ass_Val_21_22, Tot_Tax_Val__20_21, Tot_Tax_Val_21_22, Est_Size, \r\n",
        "                         Org_Construct_Yr, Last_Sale_Price, Month_YR, Sale_Type, Land_use, Dwelling_unit, Floor1_sqr, Floor2_sqr, Floor3_sqr,     \r\n",
        "                        Unfin_base, Fin_base, Base_Garage, Tot_Garage, Casita_sq, Carport, Style, Bedrooms, Bathrooms,ADDN_CONV, Pool, Spa, Construct_type,\r\n",
        "                       Roof_type, Fireplace]\r\n",
        "\r\n",
        "  if Results_Object == None :\r\n",
        "    Results_Object = Row_list # To start first row\r\n",
        "  else: #Append the new row 2nd, 3rd, etc.\r\n",
        "    Results_Object.append(Row_list)\r\n",
        "\r\n",
        "#Create dataframe object from list of lists created by loop\r\n",
        "Web_Scrape_Results = pd.DataFrame.from_records(Results_Object)\r\n",
        "#Create Excel Writer object for file title\r\n",
        "writer = pd.ExcelWriter('Washoe_Web_Scrape_Full_scale.xlsx')\r\n",
        "#Write dataframe to excel\r\n",
        "Web_Scrape_Results.to_excel(writer)\r\n",
        "#Save the Excel\r\n",
        "writer.save()\r\n",
        "print('Dataframe successfully written to Excel')\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD7Qi6fIxWiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb4edf3f-ed6e-4f5b-c2ac-44975cd71423"
      },
      "source": [
        "# Run batches of APNs, scrape all attributes, but loop full APN list in batches\r\n",
        "\r\n",
        "import requests\r\n",
        "from bs4 import BeautifulSoup as bs \r\n",
        "import re \r\n",
        "import pandas as pd \r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "os.getcwd()\r\n",
        "import string\r\n",
        "#!pip install xlsxwriter\r\n",
        "import xlsxwriter\r\n",
        "import itertools\r\n",
        "\r\n",
        "#UPLOAD APN URL FILE\r\n",
        "APN_URL_sample = []\r\n",
        "APN_URL_sample  = pd.read_excel('/content/APN URL VECTOR.xlsx')\r\n",
        "APN_URL_list = list(APN_URL_sample ['APN URL'])\r\n",
        "\r\n",
        "Results_Object = None\r\n",
        "\r\n",
        "for URL in itertools.islice(APN_URL_list,0,2550):\r\n",
        "  #Connect to URL for HTML page source code\r\n",
        "  load = requests.get(URL)\r\n",
        "  #Convert to a beautiful soup object\r\n",
        "  soup = bs(load.content)\r\n",
        "\r\n",
        "  ###################################################\r\n",
        "  #Scrape values from HTML using id\r\n",
        "  APN_value = soup.find(id = \"lblParcel\").contents\r\n",
        "  Owner = soup.find(id = \"lblOwner1\").contents \r\n",
        "\r\n",
        "  #ADDRESS \r\n",
        "  Address1 = soup.find(id = \"lblAddr1\").contents \r\n",
        "  Address2 = soup.find(id = \"lblAddr2\").contents \r\n",
        "  Address3 = soup.find(id = \"lblAddr3\").contents \r\n",
        "  Address4 = soup.find(id = \"lblAddr4\").contents \r\n",
        "  Address5 = soup.find(id = \"lblAddr5\").contents \r\n",
        "\r\n",
        "  Location_Add = soup.find(id = \"lblLocation\").contents  \r\n",
        "  City_Unincorp_town = soup.find(id = \"lblTown\").contents \r\n",
        "\r\n",
        "  Assess_Desc1 = soup.find(id = \"lblDesc1\").contents \r\n",
        "  Assess_Desc2 = soup.find(id = \"lblDesc2\").contents \r\n",
        "  Assess_Desc3 = soup.find(id = \"lblDesc3\").contents \r\n",
        "\r\n",
        "  Record_doc_No = soup.find(id = \"RecDoc\").contents  \r\n",
        "  Record_date =  soup.find(id = \"lblRecDate\").contents\r\n",
        "  Vesting =  soup.find(id = \"lblVest\").contents\r\n",
        "\r\n",
        "  #Appraisal\r\n",
        "  Tax_District = soup.find(id = \"lblTaxDist\").contents \r\n",
        "  Appraisal_YR = soup.find(id = \"lblApprYr\").contents \r\n",
        "\r\n",
        "  #REAL PROPERTY ASSESSED VALUE  (Two Fiscal periods 2020-2021 and 2021-2022)\r\n",
        "  Land_20_21 = soup.find(id = \"lblLand1\").contents \r\n",
        "  Land_21_22 = soup.find(id = \"lblLand2\").contents \r\n",
        "  #\r\n",
        "  Improvements_20_21 = soup.find(id = \"lblLand1\").contents \r\n",
        "  Improvements_21_22 = soup.find(id = \"lblLand2\").contents \r\n",
        "  #\r\n",
        "  Gross_Assessed_Sub_20_21 = soup.find(id = \"lblGross1\").contents \r\n",
        "  Gross_Assessed_Sub_21_22 = soup.find(id = \"lblGross2\").contents \r\n",
        "  #\r\n",
        "  Taxable_LnI_20_21 = soup.find(id = \"lblTaxVal1\").contents \r\n",
        "  Taxable_LnI_21_22 = soup.find(id = \"lblTaxVal2\").contents \r\n",
        "  #\r\n",
        "  Tot_Ass_Val_20_21 = soup.find(id = \"lblTAssessed1\").contents \r\n",
        "  Tot_Ass_Val_21_22 = soup.find(id = \"lblTAssessed2\").contents \r\n",
        "  #\r\n",
        "  Tot_Tax_Val__20_21 = soup.find(id = \"lblTTaxable1\").contents \r\n",
        "  Tot_Tax_Val_21_22  = soup.find(id = \"lblTTaxable2\").contents \r\n",
        "\r\n",
        "  #Estimated Lot Size and Apprasial Info\r\n",
        "  Est_Size = soup.find(id = \"lblAcres\").contents \r\n",
        "  Org_Construct_Yr  = soup.find(id = \"lblConstrYr\").contents \r\n",
        "  Last_Sale_Price = soup.find(id = \"lblSalePrice\").contents \r\n",
        "  Month_YR = soup.find(id = \"lblSaleDate\").contents \r\n",
        "  Sale_Type = soup.find(id = \"lblSaleType\").contents \r\n",
        "  Land_use =soup.find(id = \"lblLandUse\").contents \r\n",
        "  Dwelling_unit = soup.find(id = \"lblUnits\").contents \r\n",
        "\r\n",
        "  #Primary Residential Structure\r\n",
        "  Floor1_sqr = soup.find(id = \"lblFirstFloor\").contents \r\n",
        "  Floor2_sqr = soup.find(id = \"lblSecondFloor\").contents \r\n",
        "  Floor3_sqr = soup.find(id = \"lblThirdFloor\").contents \r\n",
        "\r\n",
        "  Unfin_base = soup.find(id = \"lblUnfinishedBasement\").contents \r\n",
        "  Fin_base = soup.find(id = \"lblFinishedBasement\").contents\r\n",
        "  Base_Garage = soup.find(id = \"lblBasementGarage\").contents \r\n",
        "  Tot_Garage = soup.find(id = \"lblGarage\").contents \r\n",
        "\r\n",
        "  Casita_sq = soup.find(id = \"lblCasita\").contents \r\n",
        "  Carport = soup.find(id = \"lblCarPort\").contents\r\n",
        "  Style = soup.find(id = \"lblStories\").contents\r\n",
        "  Bedrooms  = soup.find(id = \"lblBedrooms\").contents\r\n",
        "  Bathrooms = soup.find(id = \"lblBath\").contents\r\n",
        "\r\n",
        "  ADDN_CONV = soup.find(id = \"lblAddition\").contents\r\n",
        "  Pool =  soup.find(id = \"lblPool\").contents\r\n",
        "  Spa = soup.find(id = \"lblSpa\").contents\r\n",
        "  Construct_type = soup.find(id = \"lblConstType\" ).contents\r\n",
        "  Roof_type = soup.find(id = \"lblRoof\" ).contents\r\n",
        "  Fireplace = soup.find(id = \"lblFireplace\").contents\r\n",
        "###################################################\r\n",
        "\r\n",
        "  #Create a list object for all the scraped items\r\n",
        "  Row_list = [APN_value, Owner, Address1,Address2,Address3,Address4,Address5, Location_Add, City_Unincorp_town, Assess_Desc1, Assess_Desc2, Assess_Desc3, Record_doc_No, Record_date, Vesting, \r\n",
        "                        Tax_District,Appraisal_YR,Land_20_21, Improvements_21_22, Gross_Assessed_Sub_20_21, Gross_Assessed_Sub_21_22 ,\r\n",
        "                        Taxable_LnI_20_21, Tot_Ass_Val_20_21, Tot_Ass_Val_21_22, Tot_Tax_Val__20_21, Tot_Tax_Val_21_22, Est_Size, \r\n",
        "                         Org_Construct_Yr, Last_Sale_Price, Month_YR, Sale_Type, Land_use, Dwelling_unit, Floor1_sqr, Floor2_sqr, Floor3_sqr,     \r\n",
        "                        Unfin_base, Fin_base, Base_Garage, Tot_Garage, Casita_sq, Carport, Style, Bedrooms, Bathrooms,ADDN_CONV, Pool, Spa, Construct_type,\r\n",
        "                       Roof_type, Fireplace]\r\n",
        "\r\n",
        "  if Results_Object == None :\r\n",
        "    Results_Object = Row_list # To start first row\r\n",
        "  else: #Append the new row 2nd, 3rd, etc.\r\n",
        "    Results_Object.append(Row_list)\r\n",
        "\r\n",
        "#Create dataframe object from list of lists created by loop\r\n",
        "Web_Scrape_Results = pd.DataFrame.from_records(Results_Object)\r\n",
        "#Create Excel Writer object for file title\r\n",
        "writer = pd.ExcelWriter('Washoe_Web_Scrape_batch_0_2550.xlsx')\r\n",
        "#Write dataframe to excel\r\n",
        "Web_Scrape_Results.to_excel(writer)\r\n",
        "#Save the Excel\r\n",
        "writer.save()\r\n",
        "print('Dataframe successfully written to Excel')\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataframe successfully written to Excel\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOaWGuSROvoM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "f1fde663-f779-4c98-c91e-d1b384a51872"
      },
      "source": [
        "# USing TRY/EXCEPT to CATCH ERRORS\r\n",
        "\r\n",
        "import requests\r\n",
        "from bs4 import BeautifulSoup as bs \r\n",
        "import re \r\n",
        "import pandas as pd \r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "os.getcwd()\r\n",
        "import string\r\n",
        "!pip install xlsxwriter\r\n",
        "import xlsxwriter\r\n",
        "import itertools\r\n",
        "\r\n",
        "#UPLOAD APN URL FILE\r\n",
        "APN_URL_sample = []\r\n",
        "APN_URL_sample  = pd.read_excel('/content/APN URL VECTOR.xlsx')\r\n",
        "APN_URL_list = list(APN_URL_sample ['APN URL'])\r\n",
        "\r\n",
        "Results_Object = None\r\n",
        "\r\n",
        "for URL in itertools.islice(APN_URL_list,751235,751240):\r\n",
        "  try:\r\n",
        "    #Connect to URL for HTML page source code\r\n",
        "    load = requests.get(URL)\r\n",
        "   #Convert to a beautiful soup object\r\n",
        "    soup = bs(load.content)\r\n",
        "\r\n",
        "    ###################################################\r\n",
        "    #Scrape values from HTML using id\r\n",
        "    APN_value = soup.find(id = \"lblParcel\").contents\r\n",
        "    Owner = soup.find(id = \"lblOwner1\").contents \r\n",
        "\r\n",
        "    #ADDRESS \r\n",
        "    Address1 = soup.find(id = \"lblAddr1\").contents \r\n",
        "    Address2 = soup.find(id = \"lblAddr2\").contents \r\n",
        "    Address3 = soup.find(id = \"lblAddr3\").contents \r\n",
        "    Address4 = soup.find(id = \"lblAddr4\").contents \r\n",
        "    Address5 = soup.find(id = \"lblAddr5\").contents \r\n",
        "\r\n",
        "    Location_Add = soup.find(id = \"lblLocation\").contents  \r\n",
        "    City_Unincorp_town = soup.find(id = \"lblTown\").contents \r\n",
        "\r\n",
        "    Assess_Desc1 = soup.find(id = \"lblDesc1\").contents \r\n",
        "    Assess_Desc2 = soup.find(id = \"lblDesc2\").contents \r\n",
        "    Assess_Desc3 = soup.find(id = \"lblDesc3\").contents \r\n",
        "\r\n",
        "    Record_doc_No = soup.find(id = \"RecDoc\").contents  \r\n",
        "    Record_date =  soup.find(id = \"lblRecDate\").contents\r\n",
        "    Vesting =  soup.find(id = \"lblVest\").contents\r\n",
        "\r\n",
        "    #Appraisal\r\n",
        "    Tax_District = soup.find(id = \"lblTaxDist\").contents \r\n",
        "    Appraisal_YR = soup.find(id = \"lblApprYr\").contents \r\n",
        "\r\n",
        "    #REAL PROPERTY ASSESSED VALUE  (Two Fiscal periods 2020-2021 and 2021-2022)\r\n",
        "    Land_20_21 = soup.find(id = \"lblLand1\").contents \r\n",
        "    Land_21_22 = soup.find(id = \"lblLand2\").contents \r\n",
        "    #\r\n",
        "    Improvements_20_21 = soup.find(id = \"lblImp1\").contents \r\n",
        "    Improvements_21_22 = soup.find(id = \"lblImp2\").contents \r\n",
        "    #\r\n",
        "    Gross_Assessed_Sub_20_21 = soup.find(id = \"lblGross1\").contents \r\n",
        "    Gross_Assessed_Sub_21_22 = soup.find(id = \"lblGross2\").contents \r\n",
        "    #\r\n",
        "    Taxable_LnI_20_21 = soup.find(id = \"lblTaxVal1\").contents \r\n",
        "    Taxable_LnI_21_22 = soup.find(id = \"lblTaxVal2\").contents \r\n",
        "    #\r\n",
        "    Tot_Ass_Val_20_21 = soup.find(id = \"lblTAssessed1\").contents \r\n",
        "    Tot_Ass_Val_21_22 = soup.find(id = \"lblTAssessed2\").contents \r\n",
        "    #\r\n",
        "    Tot_Tax_Val__20_21 = soup.find(id = \"lblTTaxable1\").contents \r\n",
        "    Tot_Tax_Val_21_22  = soup.find(id = \"lblTTaxable2\").contents \r\n",
        "\r\n",
        "    #Estimated Lot Size and Apprasial Info\r\n",
        "    Est_Size = soup.find(id = \"lblAcres\").contents \r\n",
        "    Org_Construct_Yr  = soup.find(id = \"lblConstrYr\").contents \r\n",
        "    Last_Sale_Price = soup.find(id = \"lblSalePrice\").contents \r\n",
        "    Month_YR = soup.find(id = \"lblSaleDate\").contents \r\n",
        "    Sale_Type = soup.find(id = \"lblSaleType\").contents \r\n",
        "    Land_use =soup.find(id = \"lblLandUse\").contents \r\n",
        "    Dwelling_unit = soup.find(id = \"lblUnits\").contents \r\n",
        "\r\n",
        "    #Primary Residential Structure  \r\n",
        "    Floor1_sqr = soup.find(id = \"lblFirstFloor\").contents \r\n",
        "    Floor2_sqr = soup.find(id = \"lblSecondFloor\").contents \r\n",
        "    Floor3_sqr = soup.find(id = \"lblThirdFloor\").contents \r\n",
        "\r\n",
        "    Unfin_base = soup.find(id = \"lblUnfinishedBasement\").contents \r\n",
        "    Fin_base = soup.find(id = \"lblFinishedBasement\").contents\r\n",
        "    Base_Garage = soup.find(id = \"lblBasementGarage\").contents \r\n",
        "    Tot_Garage = soup.find(id = \"lblGarage\").contents \r\n",
        "\r\n",
        "    Casita_sq = soup.find(id = \"lblCasita\").contents \r\n",
        "    Carport = soup.find(id = \"lblCarPort\").contents\r\n",
        "    Style = soup.find(id = \"lblStories\").contents\r\n",
        "    Bedrooms  = soup.find(id = \"lblBedrooms\").contents\r\n",
        "    Bathrooms = soup.find(id = \"lblBath\").contents\r\n",
        "\r\n",
        "    ADDN_CONV = soup.find(id = \"lblAddition\").contents\r\n",
        "    Pool =  soup.find(id = \"lblPool\").contents\r\n",
        "    Spa = soup.find(id = \"lblSpa\").contents\r\n",
        "    Construct_type = soup.find(id = \"lblConstType\" ).contents\r\n",
        "    Roof_type = soup.find(id = \"lblRoof\" ).contents\r\n",
        "    Fireplace = soup.find(id = \"lblFireplace\").contents\r\n",
        "###################################################\r\n",
        "\r\n",
        "    #Create a list object for all the scraped items\r\n",
        "    Row_list = [APN_value, Owner, Address1,Address2,Address3,Address4,Address5, Location_Add, City_Unincorp_town, Assess_Desc1, Assess_Desc2, Assess_Desc3, Record_doc_No, Record_date, Vesting, \r\n",
        "                        Tax_District,Appraisal_YR,Land_20_21,Land_21_22, Improvements_20_21, Improvements_21_22, Gross_Assessed_Sub_20_21, Gross_Assessed_Sub_21_22 ,\r\n",
        "                        Taxable_LnI_20_21, Taxable_LnI_21_22, Tot_Ass_Val_20_21, Tot_Ass_Val_21_22, Tot_Tax_Val__20_21, Tot_Tax_Val_21_22, Est_Size, \r\n",
        "                         Org_Construct_Yr, Last_Sale_Price, Month_YR, Sale_Type, Land_use, Dwelling_unit, Floor1_sqr, Floor2_sqr, Floor3_sqr,     \r\n",
        "                        Unfin_base, Fin_base, Base_Garage, Tot_Garage, Casita_sq, Carport, Style, Bedrooms, Bathrooms,ADDN_CONV, Pool, Spa, Construct_type,\r\n",
        "                       Roof_type, Fireplace]\r\n",
        "\r\n",
        "    if Results_Object == None :\r\n",
        "      Results_Object = Row_list # To start first row\r\n",
        "    else: #Append the new row 2nd, 3rd, etc.\r\n",
        "      Results_Object.append(Row_list)\r\n",
        "\r\n",
        "  except Exception as ex:\r\n",
        "    pass\r\n",
        "\r\n",
        "\r\n",
        "#Create dataframe object from list of lists created by loop\r\n",
        "Web_Scrape_Results = pd.DataFrame.from_records(Results_Object)\r\n",
        "#Create Excel Writer object for file title\r\n",
        "writer = pd.ExcelWriter('Clark_Web_Scrape_Exceptions_passed_751235_751240.xlsx')\r\n",
        "#Write dataframe to excel\r\n",
        "Web_Scrape_Results.to_excel(writer)\r\n",
        "#Save the Excel\r\n",
        "writer.save()\r\n",
        "print('Dataframe successfully written to Excel')\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.6/dist-packages (1.3.7)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-46130fa95fea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;31m#Create dataframe object from list of lists created by loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m \u001b[0mWeb_Scrape_Results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResults_Object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;31m#Create Excel Writer object for file title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExcelWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Clark_Web_Scrape_Exceptions_passed_751235_751240.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mfrom_records\u001b[0;34m(cls, data, index, exclude, columns, coerce_float, nrows)\u001b[0m\n\u001b[1;32m   1789\u001b[0m             \u001b[0marr_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1790\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1791\u001b[0;31m             \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1793\u001b[0m             \u001b[0marr_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDyd4yaC-Lf2",
        "outputId": "cdf1a5fe-29c2-4bb7-ff18-ce46b52065a9"
      },
      "source": [
        "###################################################\r\n",
        "#CLEAN WEB SCRAPPED OUTPUT FILES\r\n",
        "###################################################\r\n",
        "import requests\r\n",
        "from bs4 import BeautifulSoup as bs \r\n",
        "import re \r\n",
        "import pandas as pd \r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "os.getcwd()\r\n",
        "import string\r\n",
        "!pip install xlsxwriter\r\n",
        "import xlsxwriter\r\n",
        "import itertools\r\n",
        "\r\n",
        "\r\n",
        "#51 columns, so need 51 temp labels\r\n",
        "Col_Names = [\"Column1\",\"Column2\",\"Column3\",\"Column4\",\"Column5\",\"Column6\",\"Column7\",\"Column8\",\"Column9\",\"Column10\",\r\n",
        "\"Column11\",\"Column12\",\"Column13\",\"Column14\",\"Column15\",\"Column16\",\"Column17\",\"Column18\",\"Column19\",\"Column20\",\r\n",
        "\"Column21\",\"Column22\",\"Column23\",\"Column24\",\"Column25\",\"Column26\",\"Column27\",\"Column28\",\"Column29\",\"Column30\",\r\n",
        "\"Column31\",\"Column32\",\"Column33\",\"Column34\",\"Column35\",\"Column36\",\"Column37\",\"Column38\",\"Column39\",\"Column40\",\r\n",
        "\"Column41\",\"Column42\",\"Column43\",\"Column44\",\"Column45\",\"Column46\",\"Column47\",\"Column48\",\"Column49\",\"Column50\",\r\n",
        "\"Column51\"]\r\n",
        "\r\n",
        "\r\n",
        "#########################################################################################################################\r\n",
        "#READ in RAW , but 1st row fixed WEB SCRAPPED FILE\r\n",
        "Rawish_Output  = pd.read_excel('/content/Clark_Web_Scrape_Exceptions_passed_560001_580000_row1_transpose.xlsx', names= Col_Names)\r\n",
        " #########################################################################################################################\r\n",
        "\r\n",
        "Rawish_Output.columns = Col_Names \r\n",
        "#Need to excape special characters using \\ . EX \\{}, \\[], or \\(\\)\r\n",
        "#Remove left bracket bullcrap\r\n",
        "Rawish_Output = Rawish_Output.replace(\"\\['\",\"\", regex = True)\r\n",
        "#Remove right bracket bullcrap\r\n",
        "Rawish_Output = Rawish_Output.replace(\"']\",\"\", regex = True) \r\n",
        "#Remove empty brackets\r\n",
        "Rawish_Output = Rawish_Output.replace(\"\\[]\",\"\", regex = True) \r\n",
        "\r\n",
        "# Seperate 5-digit ZIP code\r\n",
        "# Create start, stop, and step variables for slice() args. Start at 2 for 2-char state,\r\n",
        "#Stop at 9 for 2 state char+ 2 spaces+5 digit zip. Step =1 since we want only 1st split\r\n",
        "start, stop, step = 2, 9, 1\r\n",
        "#Convert ZIP column to string type\r\n",
        "Rawish_Output[\"Column5\"]= Rawish_Output[\"Column5\"].astype(str)\r\n",
        "#Slice n' Dice!\r\n",
        "Rawish_Output['5 digit ZIP'] = Rawish_Output['Column5'].str.slice(start, stop, step)\r\n",
        "\r\n",
        "# Seperate 2-char state\r\n",
        "start, stop, step = 0, 2, 1\r\n",
        "#Convert ZIP column to string type\r\n",
        "Rawish_Output[\"Column5\"]= Rawish_Output[\"Column5\"].astype(str)\r\n",
        "#Slice n' Dice!\r\n",
        "Rawish_Output['State'] = Rawish_Output['Column5'].str.slice(start, stop, step)\r\n",
        "\r\n",
        "\r\n",
        "#Check\r\n",
        "#Rawish_Output.head(n=5)\r\n",
        "\r\n",
        "#Create Excel Writer object for file title\r\n",
        "writer = pd.ExcelWriter('Clark_Web_Scr_Exc_pass_Clean_560001_580000_row1_transpose.xlsx')\r\n",
        "#Write dataframe to excel\r\n",
        "Rawish_Output.to_excel(writer)\r\n",
        "#Save the Excel\r\n",
        "writer.save()\r\n",
        "print('Dataframe successfully written to Excel')\r\n",
        "\r\n",
        "\r\n",
        "#NO_Left_bracket = Rawish_Output.str.replace(\"['\",\"\")\r\n",
        "#NO_Left_bracket.head(n=10) \r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.6/dist-packages (1.3.7)\n",
            "Dataframe successfully written to Excel\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kgqEZ5zbw1L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab5b2158-a6d6-44b5-d900-2f048858fdb1"
      },
      "source": [
        "###################################################\r\n",
        "#APPEND ALL CLEANED FILES\r\n",
        "###################################################\r\n",
        "import requests\r\n",
        "from bs4 import BeautifulSoup as bs \r\n",
        "import re \r\n",
        "import pandas as pd \r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "os.getcwd()\r\n",
        "import string\r\n",
        "!pip install xlsxwriter\r\n",
        "import xlsxwriter\r\n",
        "import itertools\r\n",
        "import glob\r\n",
        "\r\n",
        "#Get filenames into a variable object\r\n",
        "path  = r'/content'\r\n",
        "All_files = glob.glob(path + \"/*.xlsx\")\r\n",
        "#create dataframe to house appended files\r\n",
        "all_data = []\r\n",
        "#Loop over files to read in and append\r\n",
        "for f in All_files:\r\n",
        " df =  pd.read_excel(f, index_col = None, header = 0)\r\n",
        " all_data.append(df)\r\n",
        "\r\n",
        "Appended_Clean = pd.concat(all_data, axis = 0, ignore_index = True)\r\n",
        "#Concatonate all datat into \r\n",
        "\r\n",
        "#Create dataframe object from list of lists created by loop\r\n",
        "Web_Scrape_Results = pd.DataFrame.from_records(Appended_Clean)\r\n",
        "#Create Excel Writer object for file title\r\n",
        "writer = pd.ExcelWriter('Clark_Web_Scrape_Clean_Appended.xlsx')\r\n",
        "#Write dataframe to excel\r\n",
        "Web_Scrape_Results.to_excel(writer)\r\n",
        "#Save the Excel\r\n",
        "writer.save()\r\n",
        "print('Dataframe successfully written to Excel')\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.6/dist-packages (1.3.7)\n",
            "Dataframe successfully written to Excel\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}